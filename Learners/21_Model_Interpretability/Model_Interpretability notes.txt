# Model_Interpretability notes.txt
üìå What is Model Interpretability?
Ability to explain or understand the internal mechanics or predictions of a machine learning model.

üîç Why is it Important?
Trust & Transparency: Understand why a model made a decision

Debugging: Find model weaknesses or biases

Regulatory compliance: Explain decisions in sensitive domains (finance, healthcare)

Feature Insights: Understand which features impact predictions

üîß Types of Interpretability Methods
Category	Description	Examples / Libraries
Intrinsic Interpretability	Models that are inherently interpretable	Linear Regression, Decision Trees, Rule-based models
Post-hoc Interpretability	Explain predictions of black-box models	LIME, SHAP, Partial Dependence Plots, Feature Importance
Global Interpretability	Understand overall model behavior	Feature importance, global SHAP values
Local Interpretability	Explain individual predictions	LIME, SHAP force plots

üß∞ Popular Tools & Techniques
LIME (Local Interpretable Model-agnostic Explanations): Explains individual predictions by approximating locally with interpretable models

SHAP (SHapley Additive exPlanations): Game-theoretic approach to assign each feature an importance value

Partial Dependence Plots (PDP): Visualize marginal effect of features

Feature Importance: Rank features by influence on model output

ICE (Individual Conditional Expectation) plots: Show effect of feature changes on prediction for individual instances

‚öôÔ∏è When to Use?
After training complex models (Random Forests, XGBoost, Neural Nets)

When you need to explain decisions to stakeholders

For debugging and improving models


The LIME explanation opens in notebook or saves as an HTML file
SHAP plots visualize both global and local explanations
SHAP requires JavaScript support for interactive plots (e.g., Jupyter notebook)


